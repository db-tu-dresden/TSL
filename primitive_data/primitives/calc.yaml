---
name: "calc"
description: "This file contains arithmetic primitives."
...
---
primitive_name: "add"
brief_description: "Adds two vector registers."
parameters:
  - ctype: "const typename Vec::register_type"
    name: "vec_a"
    description: "First vector."
  - ctype: "const typename Vec::register_type"
    name: "vec_b"
    description: "Second vector."
returns:
  ctype: "typename Vec::register_type"
  description: "Vector containing result of the addition."
testing: #optional
  - test_name: "zero_cornercase"
    requires: ["set1", "loadu", "storeu"]
    includes: ["<cstddef>"]
    implementation: |
      using T = typename Vec::base_type;
      std::size_t element_count = 1024;
      testing::test_memory_helper_t<Vec> test_helper{element_count, Vec::vector_element_count(), false};
      bool allOk = true;
      auto reference_data_ptr = test_helper.data_ref();
      auto reference_result_ptr = test_helper.result_ref();
      auto test_data_ptr = test_helper.data_target();
      auto test_result_ptr = test_helper.result_target();
      for(std::size_t i = 0; i < element_count - Vec::vector_element_count(); i+=Vec::vector_element_count()) {
        std::size_t tester_idx = 0;
        for(size_t j = i; j < i + Vec::vector_element_count(); ++j) {
            reference_result_ptr[tester_idx++] = reference_data_ptr[j];
        }
        auto vec = set1<Vec>( 0 );
        auto elements = loadu<Vec>(&test_data_ptr[i]);
        vec = add<Vec>(vec, elements);
        storeu<Vec>( test_result_ptr, vec );
        test_helper.synchronize();
        allOk &= test_helper.validate();
      }
      return allOk;
  - test_name: "running_sum_w_epsilon"
    requires: [ "loadu", "storeu"]
    includes: ["<cstddef>"]
    implementation: |
      using T = typename Vec::base_type;
      std::size_t element_count = 1024;
      testing::test_memory_helper_t<Vec> test_helper{element_count, Vec::vector_element_count(), false };
      bool allOk = true;
      auto reference_data_ptr = test_helper.data_ref();
      auto reference_result_ptr = test_helper.result_ref();
      auto test_data_ptr = test_helper.data_target();
      auto test_result_ptr = test_helper.result_target();
      for(std::size_t i = 0; i < element_count - 2*Vec::vector_element_count(); i+=2*Vec::vector_element_count()) {
        std::size_t tester_idx = 0;
        for(size_t j = i; j < i + Vec::vector_element_count(); j++) {
            reference_result_ptr[tester_idx++] = reference_data_ptr[j]+reference_data_ptr[j+Vec::vector_element_count()];
        }
        auto elements_vec1 = loadu<Vec>(&test_data_ptr[i]);
        auto elements_vec2 = loadu<Vec>(&test_data_ptr[i+Vec::vector_element_count()]);
        auto vec = add<Vec>(elements_vec1, elements_vec2);
        storeu<Vec>( test_result_ptr, vec );
        test_helper.synchronize();
        allOk &= test_helper.validate();
      }
      return allOk;
definitions:
#CUDA
  - target_extension: "cuda"
    ctype: ["uint8_t", "uint16_t", "uint32_t", "uint64_t", "int8_t", "int16_t", "int32_t", "int64_t", "float", "double"]
    lscpu_flags: ["cuda"]
    vector_length_agnostic: True
    implementation: |
      typename Vec::register_type vec_c;
      size_t element_count = VectorSize / (sizeof({{ ctype }}) * 8);
      constexpr auto add = +[]({{ ctype }} a, {{ ctype }} b) { return a + b; };
      return launch_elemenwise_op<typename Vec::register_type, add>(vec_a, vec_b, VectorSize);
#INTEL - AVX512
  - target_extension: "avx512"
    ctype: ["uint32_t", "uint64_t", "int32_t", "int64_t"]
    lscpu_flags: ['avx512f']
    specialization_comment: "Signed addition."
    implementation: "return _mm512_add_epi{{ intrin_tp[ctype][1] }}(vec_a, vec_b);"
  - target_extension: "avx512"
    ctype: ["uint8_t", "uint16_t", "int8_t", "int16_t"]
    lscpu_flags: ['avx512f', 'avx512bw']
    specialization_comment: "Signed addition."
    implementation: "return _mm512_add_epi{{ intrin_tp[ctype][1] }}(vec_a, vec_b);"
  - target_extension: "avx512"
    ctype: ["float", "double"]
    lscpu_flags: ['avx512f']
    implementation: "return _mm512_add_{{ intrin_tp_full[ctype] }}(vec_a, vec_b);"
#INTEL - AVX2
  - target_extension: "avx2"
    ctype: ["uint8_t", "uint16_t", "uint32_t", "uint64_t", "int8_t", "int16_t", "int32_t", "int64_t"]
    lscpu_flags: ['avx2']
    specialization_comment: "Signed addition."
    implementation: "return _mm256_add_epi{{ intrin_tp[ctype][1] }}(vec_a, vec_b);"
  - target_extension: "avx2"
    ctype: ["float", "double"]
    lscpu_flags: ['avx']
    implementation: "return _mm256_add_{{ intrin_tp_full[ctype] }}(vec_a, vec_b);"
#INTEL - SSE
  - target_extension: "sse"
    ctype: ["uint8_t", "uint16_t", "uint32_t", "uint64_t", "int8_t", "int16_t", "int32_t", "int64_t"]
    lscpu_flags: ['sse2']
    specialization_comment: "Signed addition."
    implementation: "return _mm_add_epi{{ intrin_tp[ctype][1] }}(vec_a, vec_b);"
  - target_extension: "sse"
    ctype: ["float"]
    lscpu_flags: ['sse']
    implementation: "return _mm_add_{{ intrin_tp_full[ctype] }}(vec_a, vec_b);"
  - target_extension: "sse"
    ctype: ["double"]
    lscpu_flags: ['sse2']
    implementation: "return _mm_add_{{ intrin_tp_full[ctype] }}(vec_a, vec_b);"
#ARM - NEON
  - target_extension: "neon"
    ctype: ["uint8_t", "uint16_t", "uint32_t", "uint64_t", "int8_t", "int16_t", "int32_t", "int64_t", "float", "double"]
    lscpu_flags: [ 'neon' ]
    implementation: "return vaddq_{{ intrin_tp_full[ctype] }}( vec_a, vec_b );"
#SCALAR
  - target_extension: "scalar"
    ctype: [ "uint8_t", "uint16_t", "uint32_t", "uint64_t", "int8_t", "int16_t", "int32_t", "int64_t", "float", "double" ]
    lscpu_flags: []
    implementation: "return vec_a + vec_b;"
#INTEL - FPGA
  - target_extension: ["oneAPIfpga", "oneAPIfpgaRTL"]
    ctype: ["uint8_t", "int8_t", "uint16_t", "int16_t", "uint32_t", "int32_t", "float", "uint64_t", "int64_t", "double"]
    lscpu_flags: ["oneAPIfpgaDev"]
    vector_length_agnostic: True
    implementation: |
      using T = typename Vec::register_type;
      T result; //initialize the result
      #pragma unroll
      for(int i = 0; i < Vec::vector_element_count(); ++i) {
        result[i] = vec_a[i] + vec_b[i];
      }
      return result;
...
---
primitive_name: "sub"
brief_description: "Subtracts two vector registers."
parameters:
  - ctype: "const typename Vec::register_type"
    name: "vec_a"
    description: "First vector."
  - ctype: "const typename Vec::register_type"
    name: "vec_b"
    description: "Second vector."
returns:
  ctype: "typename Vec::register_type"
  description: "Vector containing result of the subtraction."
testing: #optional
  - test_name: "zero_cornercase"
    requires: ["set1", "loadu", "storeu"]
    includes: ["<cstddef>"]
    implementation: |
      using T = typename Vec::base_type;
      std::size_t element_count = 1024;
      testing::test_memory_helper_t<Vec> test_helper{element_count, Vec::vector_element_count(), false};
      bool allOk = true;
      auto reference_data_ptr = test_helper.data_ref();
      auto reference_result_ptr = test_helper.result_ref();
      auto test_data_ptr = test_helper.data_target();
      auto test_result_ptr = test_helper.result_target();
      for(std::size_t i = 0; i < element_count - Vec::vector_element_count(); i+=Vec::vector_element_count()) {
          std::size_t tester_idx = 0;
          for(size_t j = i; j < i + Vec::vector_element_count(); ++j) {
            reference_result_ptr[tester_idx++] = reference_data_ptr[j];
          }
          auto vec = set1<Vec>( 0 );
          auto elements = loadu<Vec>(&test_data_ptr[i]);
          vec = sub<Vec>(elements, vec);
          storeu<Vec>( test_result_ptr, vec );
          test_helper.synchronize();
          allOk &= test_helper.validate();
      }
      return allOk;
  - test_name: "running_sum_w_epsilon"
    requires: ["loadu", "store"]
    includes: ["<cstddef>"]
    implementation: |
      using T = typename Vec::base_type;
      std::size_t element_count = 1024;
      testing::test_memory_helper_t<Vec> test_helper{element_count, Vec::vector_element_count(), false };
      bool allOk = true;
      auto reference_data_ptr = test_helper.data_ref();
      auto reference_result_ptr = test_helper.result_ref();
      auto test_data_ptr = test_helper.data_target();
      auto test_result_ptr = test_helper.result_target();
      for(std::size_t i = 0; i < element_count - 2*Vec::vector_element_count(); i+=2*Vec::vector_element_count()) {
          std::size_t tester_idx = 0;
          for(size_t j = i; j < i + Vec::vector_element_count(); j++) {
            reference_result_ptr[tester_idx++] = reference_data_ptr[j]-reference_data_ptr[j+Vec::vector_element_count()];
          }
          auto elements_vec1 = loadu<Vec>(&test_data_ptr[i]);
          auto elements_vec2 = loadu<Vec>(&test_data_ptr[i+Vec::vector_element_count()]);
          auto vec = sub<Vec>(elements_vec1, elements_vec2);
          storeu<Vec>( test_result_ptr, vec );
          test_helper.synchronize();
          allOk &= test_helper.validate();
      }
      return allOk;
definitions:
#CUDA
  - target_extension: "cuda"
    ctype: ["uint8_t", "uint16_t", "uint32_t", "uint64_t", "int8_t", "int16_t", "int32_t", "int64_t", "float", "double"]
    lscpu_flags: ["cuda"]
    vector_length_agnostic: True
    implementation: |
      typename Vec::register_type vec_c;
      size_t element_count = VectorSize / (sizeof({{ ctype }}) * 8);
      constexpr auto add = +[]({{ ctype }} a, {{ ctype }} b) { return a - b; };
      return launch_elemenwise_op<typename Vec::register_type, add>(vec_a, vec_b, VectorSize);
#INTEL - AVX512
  - target_extension: "avx512"
    ctype: ["uint32_t", "uint64_t", "int32_t", "int64_t"]
    lscpu_flags: ['avx512f']
    specialization_comment: "Signed addition."
    implementation: "return _mm512_sub_epi{{ intrin_tp[ctype][1] }}(vec_a, vec_b);"
  - target_extension: "avx512"
    ctype: ["uint8_t", "uint16_t", "int8_t", "int16_t"]
    lscpu_flags: ['avx512f', 'avx512bw']
    specialization_comment: "Signed addition."
    implementation: "return _mm512_sub_epi{{ intrin_tp[ctype][1] }}(vec_a, vec_b);"
  - target_extension: "avx512"
    ctype: ["float", "double"]
    lscpu_flags: ['avx512f']
    implementation: "return _mm512_sub_{{ intrin_tp_full[ctype] }}(vec_a, vec_b);"
#INTEL - AVX2
  - target_extension: "avx2"
    ctype: ["uint8_t", "uint16_t", "uint32_t", "uint64_t", "int8_t", "int16_t", "int32_t", "int64_t"]
    lscpu_flags: ['avx2']
    specialization_comment: "Signed addition."
    implementation: "return _mm256_sub_epi{{ intrin_tp[ctype][1] }}(vec_a, vec_b);"
  - target_extension: "avx2"
    ctype: ["float", "double"]
    lscpu_flags: ['avx']
    implementation: "return _mm256_sub_{{ intrin_tp_full[ctype] }}(vec_a, vec_b);"
#INTEL - SSE
  - target_extension: "sse"
    ctype: ["uint8_t", "uint16_t", "uint32_t", "uint64_t", "int8_t", "int16_t", "int32_t", "int64_t"]
    lscpu_flags: ['sse2']
    specialization_comment: "Signed addition."
    implementation: "return _mm_sub_epi{{ intrin_tp[ctype][1] }}(vec_a, vec_b);"
  - target_extension: "sse"
    ctype: ["float"]
    lscpu_flags: ['sse']
    implementation: "return _mm_sub_{{ intrin_tp_full[ctype] }}(vec_a, vec_b);"
  - target_extension: "sse"
    ctype: ["double"]
    lscpu_flags: ['sse2']
    implementation: "return _mm_sub_{{ intrin_tp_full[ctype] }}(vec_a, vec_b);"
#ARM - NEON
  - target_extension: "neon"
    ctype: ["uint8_t", "uint16_t", "uint32_t", "uint64_t", "int8_t", "int16_t", "int32_t", "int64_t", "float", "double"]
    lscpu_flags: [ 'neon' ]
    implementation: "return vsubq_{{ intrin_tp_full[ctype] }}( vec_a, vec_b );"
#SCALAR
  - target_extension: "scalar"
    ctype: [ "uint8_t", "uint16_t", "uint32_t", "uint64_t", "int8_t", "int16_t", "int32_t", "int64_t", "float", "double" ]
    lscpu_flags: []
    implementation: "return vec_a - vec_b;"
#INTEL - FPGA
  - target_extension: ["oneAPIfpga", "oneAPIfpgaRTL"]
    ctype: ["uint8_t", "int8_t", "uint16_t", "int16_t", "uint32_t", "int32_t", "float", "uint64_t", "int64_t", "double"]
    lscpu_flags: ["oneAPIfpgaDev"]
    vector_length_agnostic: True
    implementation: |
      using T = typename Vec::register_type;
      T result; //initialize the result
      #pragma unroll
      for(int i = 0; i < Vec::vector_element_count(); ++i) {
        result[i] = vec_a[i] - vec_b[i];
      }
      return result;
---
primitive_name: "add"
functor_name: "mask_add"
brief_description: "Adds two vector registers, depending on a mask: result[*] = (m[*])? vec_a[*]+vec_b[*] : vec_a[*]."
parameters:
  - ctype: "const typename Vec::mask_type"
    name: "mask"
    description: "Vector mask register indicating which elements should be added."
  - ctype: "const typename Vec::register_type"
    name: "vec_a"
    description: "First vector."
  - ctype: "const typename Vec::register_type"
    name: "vec_b"
    description: "Second vector."
returns:
  ctype: "typename Vec::register_type"
  description: "Vector containing result of the addition."
definitions:
#INTEL - AVX512
  - target_extension: "avx512"
    ctype: ["float", "double"]
    lscpu_flags: ["avx512f"]
    implementation: "return _mm512_mask_add_{{ intrin_tp_full[ctype] }}(vec_a, mask, vec_a, vec_b);"
#INTEL - AVX2
  - target_extension: "avx2"
    ctype: ["uint8_t", "int8_t", "uint16_t", "int16_t", "uint32_t", "int32_t", "uint64_t", "int64_t"]
    lscpu_flags: ["avx"]
    implementation: "return _mm256_add_epi{{ intrin_tp[ctype][1] }}(vec_a, _mm256_and_si256(vec_b, mask));"
  - target_extension: "avx2"
    ctype: ["float", "double"]
    lscpu_flags: ["avx"]
    implementation: "return _mm256_add_{{ intrin_tp_full[ctype] }}(vec_a, _mm256_and_{{ intrin_tp_full[ctype] }}(vec_b, mask));"
#INTEL - SSE
  - target_extension: "sse"
    ctype: ["uint8_t", "int8_t", "uint16_t", "int16_t", "uint32_t", "int32_t", "uint64_t", "int64_t"]
    lscpu_flags: ["sse2"]
    implementation: "return _mm_add_epi{{ intrin_tp[ctype][1] }}(vec_a, _mm_and_si128(vec_b, mask));"
  - target_extension: "sse"
    ctype: ["float"]
    lscpu_flags: ["sse"]
    implementation: "return _mm_add_ps(vec_a, _mm_and_ps(vec_b, mask));"
  - target_extension: "sse"
    ctype: ["double"]
    lscpu_flags: ["sse2"]
    implementation: "return _mm_add_pd(vec_a, _mm_and_pd(vec_b, mask));"
---
primitive_name: "mul"
brief_description: "Multiplies two vector registers."
parameters:
  - ctype: "const typename Vec::register_type"
    name: "vec_a"
    description: "First vector."
  - ctype: "const typename Vec::register_type"
    name: "vec_b"
    description: "Second vector."
returns:
  ctype: "typename Vec::register_type"
  description: "Vector containing result of the multiplication."
testing:
  - requires: ["loadu", "storeu"]
    includes: ["<cstddef>"]
    implementation: |
        using T = typename Vec::base_type;
        std::size_t element_count = 1024;
        testing::test_memory_helper_t<Vec> test_helper{element_count, Vec::vector_element_count(), false };
        bool allOk = true;
        auto reference_data_ptr = test_helper.data_ref();
        auto reference_result_ptr = test_helper.result_ref();
        auto test_data_ptr = test_helper.data_target();
        auto test_result_ptr = test_helper.result_target();
        for(std::size_t i = 0; i < element_count - (2*Vec::vector_element_count()); i+=(2*Vec::vector_element_count())) {
          std::size_t j = i;
          for(; j < i + Vec::vector_element_count(); ++j) {
              reference_result_ptr[j-i] = reference_data_ptr[j];
          }
          for(; j < i + (2*Vec::vector_element_count()); ++j) {
              reference_result_ptr[j-(i+Vec::vector_element_count())] *= reference_data_ptr[j];
          }
          auto vec_a = loadu<Vec>(&test_data_ptr[i]);
          auto vec_b = loadu<Vec>(&test_data_ptr[i+Vec::vector_element_count()]);
          auto vec_result = mul<Vec>(vec_a, vec_b);
          storeu<Vec>(test_result_ptr, vec_result);
          test_helper.synchronize();
          allOk &= test_helper.validate();
        }
        return allOk;
definitions:
#INTEL - AVX512
  - target_extension: "avx512"
    ctype: ["float", "double"]
    lscpu_flags: ["avx512f"]
    implementation: "return _mm512_mul_{{ intrin_tp_full[ctype] }}(vec_a, vec_b);"
  - target_extension: "avx512"
    ctype: ["uint16_t"]
    lscpu_flags: ["avx512bw"]
    specialization_comment: "Signed multiplication."
    implementation: "return _mm512_mullo_epi16(vec_a, vec_b);"
  - target_extension: "avx512"
    ctype: ["int16_t"]
    lscpu_flags: ["avx512bw"]
    implementation: "return _mm512_mullo_epi16(vec_a, vec_b);"
  - target_extension: "avx512"
    ctype: ["uint32_t", "int32_t"]
    lscpu_flags: ["avx512f"]
    specialization_comment: "Signed multiplication."
    implementation: "return _mm512_mullo_epi32(vec_a, vec_b);"
  - target_extension: "avx512"
    ctype: ["uint64_t", "int64_t"]
    lscpu_flags: ["avx512dq"]
    specialization_comment: "Signed multiplication."
    implementation: "return _mm512_mullo_epi64(vec_a, vec_b);"
  - target_extension: "avx512"
    ctype: ["uint8_t", "int8_t", "uint16_t", "int16_t", "uint32_t", "int32_t", "uint64_t", "int64_t"]
    lscpu_flags: ["avx512f"]
    is_native: False
    includes: ["<array>", "<cstddef>"]
    implementation: |
      alignas(Vec::vector_alignment()) std::array<typename Vec::base_type, Vec::vector_element_count()> buffer_a;
      alignas(Vec::vector_alignment()) std::array<typename Vec::base_type, Vec::vector_element_count()> buffer_b;
      _mm512_store_si512(reinterpret_cast<void*>(buffer_a.data()), vec_a);
      _mm512_store_si512(reinterpret_cast<void*>(buffer_b.data()), vec_b);
      for(std::size_t i = 0; i < Vec::vector_element_count(); ++i) {
          buffer_a[i] *= buffer_b[i];
      }
      return _mm512_load_si512(reinterpret_cast<void const *>(buffer_a.data()));
#INTEL - AVX2
  - target_extension: "avx2"
    ctype: ["float", "double"]
    lscpu_flags: ["avx"]
    implementation: "return _mm256_mul_{{ intrin_tp_full[ctype] }}(vec_a, vec_b);"
  - target_extension: "avx2"
    ctype: ["uint16_t", "int16_t"]
    lscpu_flags: ["avx2"]
    specialization_comment: "Signed multiplication."
    implementation: "return _mm256_mullo_epi16(vec_a, vec_b);"
  - target_extension: "avx2"
    ctype: ["uint32_t", "int32_t"]
    lscpu_flags: ["avx2"]
    specialization_comment: "Signed multiplication."
    implementation: "return _mm256_mullo_epi32(vec_a, vec_b);"
  - target_extension: "avx2"
    ctype: ["uint64_t", "int64_t"]
    lscpu_flags: ["avx512dq", "avx512vl"]
    specialization_comment: "Signed multiplication."
    implementation: "return _mm256_mullo_epi64(vec_a, vec_b);"
  - target_extension: "avx2"
    ctype: ["uint8_t", "int8_t", "uint16_t", "int16_t", "uint32_t", "int32_t", "uint64_t", "int64_t"]
    lscpu_flags: ["avx"]
    is_native: False
    includes: ["<array>", "<cstddef>"]
    implementation: |
      alignas(Vec::vector_alignment()) std::array<typename Vec::base_type, Vec::vector_element_count()> buffer_a;
      alignas(Vec::vector_alignment()) std::array<typename Vec::base_type, Vec::vector_element_count()> buffer_b;
      _mm256_store_si256(reinterpret_cast<__m256i*>(buffer_a.data()), vec_a);
      _mm256_store_si256(reinterpret_cast<__m256i*>(buffer_b.data()), vec_b);
      for(std::size_t i = 0; i < Vec::vector_element_count(); ++i) {
          buffer_a[i] *= buffer_b[i];
      }
      return _mm256_load_si256(reinterpret_cast<__m256i const *>(buffer_a.data()));
#INTEL - SSE
  - target_extension: "sse"
    ctype: ["float"]
    lscpu_flags: ["sse"]
    implementation: "return _mm_mul_ps(vec_a, vec_b);"
  - target_extension: "sse"
    ctype: ["double"]
    lscpu_flags: ["sse2"]
    implementation: "return _mm_mul_pd(vec_a, vec_b);"
  - target_extension: "sse"
    ctype: ["uint16_t", "int16_t"]
    lscpu_flags: ["sse2"]
    specialization_comment: "Signed multiplication."
    implementation: "return _mm_mullo_epi16(vec_a, vec_b);"
  - target_extension: "sse"
    ctype: ["uint32_t", "int32_t"]
    lscpu_flags: ["sse4_1"]
    specialization_comment: "Signed multiplication."
    implementation: "return _mm_mullo_epi32(vec_a, vec_b);"
  - target_extension: "sse"
    ctype: ["uint64_t", "int64_t"]
    lscpu_flags: ["avx512dq", "avx512vl"]
    specialization_comment: "Signed multiplication."
    implementation: "return _mm_mullo_epi64(vec_a, vec_b);"
  - target_extension: ["sse"]
    ctype: ["uint64_t", "int64_t"]
    lscpu_flags: ["sse2"]
    implementation: |
      __m128i aHigh = _mm_shuffle_epi32(vec_a, _MM_SHUFFLE(3, 2, 3, 2));
      __m128i aLow = _mm_shuffle_epi32(vec_a, _MM_SHUFFLE(1, 0, 1, 0));
      __m128i bHigh = _mm_shuffle_epi32(vec_b, _MM_SHUFFLE(3, 2, 3, 2));
      __m128i bLow = _mm_shuffle_epi32(vec_b, _MM_SHUFFLE(1, 0, 1, 0));

      // Multiply 32-bit parts
      __m128i prodLow = _mm_mul_epu32(aLow, bLow);
      __m128i prodHigh = _mm_mul_epu32(aHigh, bHigh);

      // Combine results
      __m128i result = _mm_set_epi64x(_mm_cvtsi128_si64(prodHigh), _mm_cvtsi128_si64(prodLow));

      return result;
  - target_extension: "sse"
    ctype: ["uint8_t", "int8_t", "uint16_t", "int16_t", "uint32_t", "int32_t", "uint64_t", "int64_t"]
    lscpu_flags: ["sse2"]
    is_native: False
    includes: ["<array>", "<cstddef>"]
    implementation: |
      alignas(Vec::vector_alignment()) std::array<typename Vec::base_type, Vec::vector_element_count()> buffer_a;
      alignas(Vec::vector_alignment()) std::array<typename Vec::base_type, Vec::vector_element_count()> buffer_b;
      _mm_store_si128(reinterpret_cast<__m128i*>(buffer_a.data()), vec_a);
      _mm_store_si128(reinterpret_cast<__m128i*>(buffer_b.data()), vec_b);
      for(std::size_t i = 0; i < Vec::vector_element_count(); ++i) {
          buffer_a[i] *= buffer_b[i];
      }
      return _mm_load_si128(reinterpret_cast<__m128i const *>(buffer_a.data()));
#ARM - NEON
  - target_extension: "neon"
    ctype: ["uint8_t", "uint16_t", "uint32_t", "int8_t", "int16_t", "int32_t", "float", "double"]
    lscpu_flags: [ 'neon' ]
    implementation: "return vmulq_{{ intrin_tp_full[ctype] }}( vec_a, vec_b );"
  - target_extension: "neon"
    ctype: ["uint64_t", "int64_t"]
    lscpu_flags: [ 'neon' ]
    is_native: False
    implementation: |
      //Found this on stackoverflow. This seems like an overkill. Maybe an extract and scalar multiply would do the trick more efficient.
      //@todo: benchmark this.
      const auto ac = vmovn_{{ intrin_tp[ctype][0] }}64(vec_a);
      const auto pr = vmovn_{{ intrin_tp[ctype][0] }}64(vec_b);
      const auto hi = vmulq_{{ intrin_tp[ctype][0] }}32(vreinterpretq_{{ intrin_tp[ctype][0] }}32_{{ intrin_tp[ctype][0] }}64(vec_b), vrev64q_{{ intrin_tp[ctype][0] }}32(vreinterpretq_{{ intrin_tp[ctype][0] }}32_{{ intrin_tp[ctype][0] }}64(vec_a)));
      return vmlal_{{ intrin_tp[ctype][0] }}32(vshlq_n_{{ intrin_tp[ctype][0] }}64(vpaddlq_{{ intrin_tp[ctype][0] }}32(hi), 32), ac, pr);
#SCALAR
  - target_extension: "scalar"
    ctype: [ "uint8_t", "uint16_t", "uint32_t", "uint64_t", "int8_t", "int16_t", "int32_t", "int64_t", "float", "double" ]
    lscpu_flags: [ ]
    implementation: "return vec_a * vec_b;"
---
primitive_name: "hadd"
brief_description: "Reduces the elements to a sum."
parameters:
  - ctype: "const typename Vec::register_type"
    name: "value"
    description: "Input vector."
returns:
  ctype: "typename Vec::base_type"
  description: "Scalar value after adding all elements in the vector."
testing:
  - requires: ["set1"]
    includes: ["<cstddef>", "<algorithm>", "<limits>"]
    implementation: |
      using T = typename Vec::base_type;
      testing::test_memory_helper_t<Vec> test_helper{1, false};
      bool allOk = true;
      auto reference_result_ptr = test_helper.result_ref();
      auto test_result_ptr = test_helper.result_target();
      const std::size_t limit = std::min( (size_t) 4096, (size_t) std::numeric_limits<T>::max() / Vec::vector_element_count() );
      for(std::size_t i = 0; i < limit; ++i) {
        *reference_result_ptr =  Vec::vector_element_count() * i;
        auto vec = set1<Vec>(i);
        *test_result_ptr = hadd<Vec>(vec);
        test_helper.synchronize();
        allOk &= test_helper.validate();
      }
      return allOk;
definitions:
#INTEL - FPGA
  - target_extension: ["oneAPIfpga", "oneAPIfpgaRTL"]
    ctype: ["uint8_t", "int8_t", "uint16_t", "int16_t", "uint32_t", "int32_t", "float"]
    lscpu_flags: ["oneAPIfpgaDev"]
    vector_length_agnostic: True
    implementation: |
      typename Vec::register_type result{};
      #pragma unroll
      for(size_t i = 0; i < Vec::vector_element_count(); ++i) { result[i] = value[i]; }
      #pragma unroll cilog2(Vec::vector_element_count())
      for(size_t current_width = (Vec::vector_element_count()>>1); current_width >= 1; current_width>>=1) {
          for(size_t i = 0; i < current_width; ++i) {
              result[i] = result[(i<<1)] + result[(i<<1)+1];
          }
      }
      return result[0];
# using T = typename Vec::base_type;
# T result = 0; //initialize the result
# #pragma unroll
# for(int i = 0; i < Vec::vector_element_count(); i+=16) {
#   T add_1_1 = value[ 0] + value[ 1];
#   T add_1_2 = value[ 2] + value[ 3];
#   T add_1_3 = value[ 4] + value[ 5];
#   T add_1_4 = value[ 6] + value[ 7];
#   T add_1_5 = value[ 8] + value[ 9];
#   T add_1_6 = value[10] + value[11];
#   T add_1_7 = value[12] + value[13];
#   T add_1_8 = value[14] + value[15];

#   T add_2_1 = add_1_1 + add_1_2;
#   T add_2_2 = add_1_3 + add_1_4;
#   T add_2_3 = add_1_5 + add_1_6;
#   T add_2_4 = add_1_7 + add_1_8;

#   T add_3_1 = add_2_1 + add_2_2;
#   T add_3_2 = add_2_3 + add_2_4;

#   result += add_3_1 + add_3_2;
# }
# return result;
  - target_extension: ["oneAPIfpga", "oneAPIfpgaRTL"]
    ctype: ["uint64_t", "int64_t", "double"]
    lscpu_flags: ["oneAPIfpgaDev"]
    vector_length_agnostic: True
    implementation: |
      using T = typename Vec::base_type;
      T result = 0; //initialize the result
      #pragma unroll
      for(int i = 0; i < Vec::vector_element_count(); i+=16) {
        T add_1_1 = value[i  ] + value[i+1];
        T add_1_2 = value[i+2] + value[i+3];
        T add_1_3 = value[i+4] + value[i+5];
        T add_1_4 = value[i+6] + value[i+7];

        T add_2_1 = add_1_1 + add_1_2;
        T add_2_2 = add_1_3 + add_1_4;

        result += add_2_1 + add_2_2;
      }
      return result;
#INTEL - AVX512
  - target_extension: "avx512"
    ctype: ["float", "double"]
    lscpu_flags: ["avx512f"]
    specialization_comment: "Be aware, that this intrinsic is flagged as 'sequence' by INTEL."
    implementation: "return _mm512_reduce_add_{{ intrin_tp_full[ctype] }}(value);"
  - target_extension: "avx512"
    ctype: ["uint32_t", "uint64_t", "int32_t", "int64_t"]
    lscpu_flags: ["avx512f"]
    specialization_comment: "Signed Addition. Be aware, that this intrinsic is flagged as 'sequence' by INTEL."
    implementation: "return _mm512_reduce_add_epi{{ intrin_tp[ctype][1] }}(value);"
  - target_extension: "avx512"
    ctype: ["uint8_t", "int8_t", "uint16_t", "int16_t"]
    lscpu_flags: ["avx512f"]
    is_native: False
    includes: ["<array>", "<cstddef>"]
    implementation: |
      alignas(Vec::vector_alignment()) std::array<typename Vec::base_type, Vec::vector_element_count()> buffer;
      typename Vec::base_type result = 0;
      _mm512_store_si512(reinterpret_cast<void*>(buffer.data()), value);
      for(std::size_t i = 0; i < Vec::vector_element_count(); ++i) {
          result += buffer[i];
      }
      return result;
#INTEL - AVX2
  - target_extension: "avx2"
    ctype: ["double"]
    specialization_comment: "This instruction needs sse3. However, most intel cpus only provide ssse3 (which is a superset sse3)."
    lscpu_flags: ["sse2", "ssse3", "avx"]
    is_native: False
    implementation: |
      //https://stackoverflow.com/questions/49941645/get-sum-of-values-stored-in-m256d-with-sse-avx
      __m128d vlow  = _mm256_castpd256_pd128(value);
      __m128d vhigh = _mm256_extractf128_pd(value, 1);
      vlow  = _mm_add_pd(vlow, vhigh);
      __m128d high64 = _mm_unpackhi_pd(vlow, vlow);
      return  _mm_cvtsd_f64(_mm_add_sd(vlow, high64));
  - target_extension: "avx2"
    ctype: ["float"]
    specialization_comment: "This instruction needs sse3. However, most intel cpus only provide ssse3 (which is a superset sse3)."
    lscpu_flags: ["sse", "sse2", "ssse3", "avx"]
    is_native: False
    implementation: |
      __m128 vlow  = _mm256_castps256_ps128(value);
      __m128 vhigh = _mm256_extractf128_ps(value, 1);
      vlow = _mm_add_ps(vlow, vhigh);
      __m128 res = _mm_hadd_ps(vlow, vlow);
      return _mm_cvtss_f32(res) + _mm_cvtss_f32(_mm_castsi128_ps(_mm_bsrli_si128(_mm_castps_si128(res),sizeof(float))));
  - target_extension: "avx2"
    ctype: ["uint64_t", "int64_t"]
    lscpu_flags: ["sse2", "avx"]
    specialization_comment: "Signed Addition."
    is_native: False
    implementation: |
      __m128i vlow = _mm256_castsi256_si128(value);
      __m128i vhigh = _mm256_extractf128_si256(value, 1);
      vlow = _mm_add_epi64(vlow, vhigh);
      __m128i high64 = _mm_unpackhi_epi64(vlow, vlow);
      return _mm_cvtsi128_si64(_mm_add_epi64(vlow, high64));
  - target_extension: "avx2"
    ctype: ["uint32_t", "int32_t"]
    specialization_comment: "Signed Addition. This instruction needs sse3. However, most intel cpus only provide ssse3 (which is a superset sse3)."
    lscpu_flags: ["sse2", "ssse3", "avx"]
    is_native: False
    implementation: |
      __m128i vlow = _mm256_castsi256_si128(value);
      __m128i vhigh = _mm256_extractf128_si256(value, 1);
      vlow = _mm_add_epi32(vlow, vhigh);
      __m128i res = _mm_hadd_epi32(vlow, vlow);
      return _mm_cvtsi128_si32(res) + _mm_cvtsi128_si32(_mm_bsrli_si128(res,sizeof(uint32_t)));
  - target_extension: "avx2"
    ctype: ["uint8_t", "int8_t", "uint16_t", "int16_t"]
    lscpu_flags: ["avx"]
    is_native: False
    includes: ["<array>", "<cstddef>"]
    implementation: |
      alignas(Vec::vector_alignment()) std::array<typename Vec::base_type, Vec::vector_element_count()> buffer;
      typename Vec::base_type result = 0;
      _mm256_store_si256(reinterpret_cast<__m256i*>(buffer.data()), value);
      for(std::size_t i = 0; i < Vec::vector_element_count(); ++i) {
        result += buffer[i];
      }
      return result;
#INTEL - SSE
  - target_extension: "sse"
    ctype: ["double"]
    lscpu_flags: ["sse2"]
    is_native: False
    implementation: |
      return _mm_cvtsd_f64(value) + _mm_cvtsd_f64(_mm_castsi128_pd(_mm_bsrli_si128(_mm_castpd_si128(value),sizeof(double))));
  - target_extension: "sse"
    ctype: ["float"]
    specialization_comment: "This instruction needs sse3. However, most intel cpus only provide ssse3 (which is a superset sse3)."
    lscpu_flags: ["sse", "sse2", "ssse3"]
    is_native: False
    implementation: |
      auto res = _mm_hadd_ps(value, value);
      return _mm_cvtss_f32(res) + _mm_cvtss_f32(_mm_castsi128_ps(_mm_bsrli_si128(_mm_castps_si128(res),sizeof(float))));
  - target_extension: "sse"
    ctype: ["uint64_t", "int64_t"]
    lscpu_flags: ["sse2", "avx"]
    specialization_comment: "Signed Addition."
    is_native: False
    implementation: |
      return _mm_cvtsi128_si64(value) + _mm_cvtsi128_si64(_mm_bsrli_si128(value,sizeof(uint64_t)));
  - target_extension: "sse"
    ctype: ["uint32_t", "int32_t"]
    lscpu_flags: ["sse2", "ssse3", "avx"]
    specialization_comment: "Signed Addition."
    is_native: False
    implementation: |
      auto res = _mm_hadd_epi32(value, value);
      return _mm_cvtsi128_si32(res) + _mm_cvtsi128_si32(_mm_bsrli_si128(res,sizeof(uint32_t)));
  - target_extension: "sse"
    ctype: ["uint8_t", "int8_t", "uint16_t", "int16_t"]
    lscpu_flags: ["sse2"]
    is_native: False
    includes: ["<array>", "<cstddef>"]
    implementation: |
      alignas(Vec::vector_alignment()) std::array<typename Vec::base_type, Vec::vector_element_count()> buffer;
      typename Vec::base_type result = 0;
      _mm_store_si128(reinterpret_cast<__m128i *>(buffer.data()), value);
      for  (std::size_t i = 0; i < Vec::vector_element_count(); ++i) {
            result += buffer[i];
      }
      return result;
#ARM - NEON
  - target_extension: "neon"
    ctype: ["uint8_t", "uint16_t", "uint32_t", "uint64_t", "int8_t", "int16_t", "int32_t", "int64_t", "float", "double"]
    lscpu_flags: [ 'neon' ]
    implementation: "return vaddvq_{{ intrin_tp_full[ctype] }}( value );"
#SCALAR
  - target_extension: "scalar"
    ctype: [ "uint8_t", "uint16_t", "uint32_t", "uint64_t", "int8_t", "int16_t", "int32_t", "int64_t", "float", "double" ]
    lscpu_flags: [ ]
    implementation: "return value;"
...
---
primitive_name: "min"
brief_description: "compares the values of 2 vectors and returns a vector with the minimum of each corrisponding values"
parameters:
  - ctype: "const typename Vec::register_type"
    name: "vec_a"
    description: "First vector"
  - ctype: "const typename Vec::register_type"
    name: "vec_b"
    description: "Second vector"
returns:
  ctype: "typename Vec::register_type"
  description: "Vector containing result of the comparison"
testing:
  - test_name: "min_general_case"
    requires: ["set1", "loadu", "storeu"]
    includes: ["<cstddef>"]
    implementation: |
        using T = typename Vec::base_type;
        const std::size_t element_count = 2048;
        testing::test_memory_helper_t<Vec> test_helper{element_count, Vec::vector_element_count(), false};
        bool allOk = true;
        auto reference_data_ptr = test_helper.data_ref();
        auto reference_result_ptr = test_helper.result_ref();
        auto test_data_ptr = test_helper.data_target();
        auto test_result_ptr = test_helper.result_target();
        auto vec = set1<Vec>(0);
        for(std::size_t i = 0; i < element_count / 2; i += Vec::vector_element_count()){
          std::size_t tester_idx = 0;
          for(size_t j = i; j < i + Vec::vector_element_count(); j++){
            if(reference_data_ptr[j] < reference_data_ptr[j + (element_count/2)]){
              reference_result_ptr[tester_idx++] = reference_data_ptr[j];
            } else {
              reference_result_ptr[tester_idx++] = reference_data_ptr[j + (element_count/2)];
            }
          }
          auto elements_vec1 = loadu<Vec>(&test_data_ptr[i]);
          auto elements_vec2 = loadu<Vec>(&test_data_ptr[i + (element_count/2)]);
          vec = min<Vec>(elements_vec1, elements_vec2);
          storeu<Vec>(test_result_ptr, vec);
          test_helper.synchronize();
          allOk &= test_helper.validate();
        }
        return allOk;
  - test_name: "min_zero_case"
    requires: ["set1", "loadu", "storeu"]
    includes: ["<cstddef>"]
    implementation: |
        using T = typename Vec::base_type;
        std::size_t element_count = 1024;
        testing::test_memory_helper_t<Vec> test_helper{element_count, Vec::vector_element_count(), false};
        bool allOk = true;
        auto reference_data_ptr = test_helper.data_ref();
        auto reference_result_ptr = test_helper.result_ref();
        auto test_data_ptr = test_helper.data_target();
        auto test_result_ptr = test_helper.result_target();
        for(std::size_t i = 0; i < element_count; i += Vec::vector_element_count()){
          auto vec = set1<Vec>(0);
          storeu<Vec>(reference_result_ptr, vec);
          auto elements_vec = loadu<Vec>(&test_data_ptr[i]);
          vec = min<Vec>(vec, elements_vec);
          storeu<Vec>(test_result_ptr, vec);
          test_helper.synchronize();
          allOk &= test_helper.validate();
        }
        return allOk;
definitions:
#INTEL - AVX512
  - target_extension: "avx512"
    ctype: ["uint32_t", "uint64_t", "int32_t", "int64_t", "float", "double"]
    lscpu_flags: ['avx512f']
    specialization_comment: "Signed Min"
    implementation: "return _mm512_min_{{ intrin_tp_full[ctype] }}(vec_a, vec_b);"
  - target_extension: "avx512"
    ctype: ["uint8_t", "uint16_t", "int8_t", "int16_t" ]
    lscpu_flags: ['avx512bw']
    specialization_comment: "Signed Min"
    implementation: "return _mm512_min_{{ intrin_tp_full[ctype] }}(vec_a, vec_b);"
#INTEL - AVX2
  - target_extension: "avx2"
    ctype: ["uint8_t", "uint16_t", "uint32_t", "int8_t", "int16_t", "int32_t"]
    lscpu_flags: ['avx2']
    specialization_comment: "Signed & unsigned Min"
    implementation: "return _mm256_min_{{ intrin_tp_full[ctype] }}(vec_a, vec_b);"
  - target_extension: "avx2"
    ctype: ["float", "double"]
    lscpu_flags: ['avx']
    implementation: "return _mm256_min_{{ intrin_tp_full[ctype] }}(vec_a, vec_b);"
  - target_extension: "avx2"
    ctype: ["int64_t", "uint64_t"]
    lscpu_flags: ['avx2']
    specialization_comment: "Takes a mask to check the smaller value of each Vector and takes that mask to get the smaller value of either vec_a or vec_b"
    is_native: False
    implementation: |
      typename Vec::register_type mask = _mm256_cmpgt_epi64(vec_a, vec_b);
      return _mm256_blendv_epi8(vec_a, vec_b, mask);
#INTEL - SSE
  - target_extension: "sse"
    ctype: ["float"]
    lscpu_flags: ['sse']
    implementation: "return _mm_min_{{intrin_tp_full[ctype]}}(vec_a, vec_b);"
  - target_extension: "sse"
    ctype: ["uint8_t", "int16_t", "double"]
    lscpu_flags: ['sse2']
    implementation: "return _mm_min_{{intrin_tp_full[ctype]}}(vec_a, vec_b);"
  - target_extension: "sse"
    ctype: ["int8_t", "uint16_t", "uint32_t", "int32_t"]
    lscpu_flags: ['sse4_1']
    implementation: "return _mm_min_{{intrin_tp_full[ctype]}}(vec_a, vec_b);"
#ARM - NEON
  - target_extension: "neon"
    ctype: [ "uint8_t", "uint16_t", "uint32_t", "int8_t", "int32_t", "double", "float" ]
    lscpu_flags: ['neon']
    implementation: "return vminq_{{intrin_tp_full[ctype]}}(vec_a, vec_b);" #What about 64-Bit
#SCALAR
  - target_extension: "scalar"
    ctype: ["uint8_t", "uint16_t", "uint32_t", "uint64_t", "int8_t", "int16_t", "int32_t", "int64_t", "float", "double"]
    lscpu_flags: []
    includes: []
    implementation: |
        if (vec_a > vec_b) return vec_b;
        return vec_a;
#INTEL - FPGA
...
---
primitive_name: "div"
brief_description: "Divides two vector registers."
parameters:
  - ctype: "const typename Vec::register_type"
    name: "vec_a"
    description: "First vector."
  - ctype: "const typename Vec::register_type"
    name: "vec_b"
    description: "Second vector."
returns:
  ctype: "typename Vec::register_type"
  description: "Vector containing result of the division."
testing:
  - test_name: "vec_with_itself"
    requires: ["loadu", "storeu", "set1"]
    implementation: |
      using T = typename Vec::base_type;
      std::size_t element_count = 1024;
      testing::test_memory_helper_t<Vec> test_helper{element_count, Vec::vector_element_count(), false, testing::alternate_init_no_zero<T>};
      bool allOk = true;
      auto reference_data_ptr = test_helper.data_ref();
      auto reference_result_ptr = test_helper.result_ref();
      auto test_data_ptr = test_helper.data_target();
      auto test_result_ptr = test_helper.result_target();
      for(std::size_t i = 0; i < element_count - Vec::vector_element_count(); i += Vec::vector_element_count()){
        auto vec = set1<Vec>(1);
        storeu<Vec>(reference_result_ptr, vec);
        auto vec_a = loadu<Vec>(&test_data_ptr[i]);
        vec = div<Vec>(vec_a, vec_a);
        storeu<Vec>(test_result_ptr, vec);
        test_helper.synchronize();
        allOk &= test_helper.validate();
      }
      return allOk;
  - test_name: "vec_with_one"
    requires: ["loadu", "storeu", "set1"]
    implementation: |
      using T = typename Vec::base_type;
      std::size_t element_count = 1024;
      testing::test_memory_helper_t<Vec> test_helper{element_count, Vec::vector_element_count(), false, testing::alternate_init_no_zero<T>};
      bool allOk = true;
      auto reference_data_ptr = test_helper.data_ref();
      auto reference_result_ptr = test_helper.result_ref();
      auto test_data_ptr = test_helper.data_target();
      auto test_result_ptr = test_helper.result_target();
      for(std::size_t i = 0; i < element_count - Vec::vector_element_count(); i += Vec::vector_element_count()){
        std::size_t tester_idx = 0;
        for(size_t j=i; j < i + Vec::vector_element_count(); j++){
          reference_result_ptr[tester_idx++] = reference_data_ptr[j];
        }
        auto vec = set1<Vec>(1);
        auto vec_a = loadu<Vec>(&test_data_ptr[i]);
        auto vec_result = div<Vec>(vec_a, vec);
        storeu<Vec>(test_result_ptr, vec_result);
        test_helper.synchronize();
        allOk &= test_helper.validate();
      }
      return allOk;
definitions:
#INTEL - AVX512
  - target_extension: "avx512"
    ctype: ["float", "double"]
    lscpu_flags: ["avx512f"]
    implementation: "return _mm512_div_{{intrin_tp_full[ctype]}}(vec_a, vec_b);"
  - target_extension: "avx512"
    ctype: ["uint8_t", "int8_t", "uint16_t", "int16_t", "uint32_t", "int32_t", "uint64_t", "int64_t"]
    lscpu_flags: ["avx512f"]
    is_native: False
    includes: ["<array>", "<cstddef>"]
    implementation: |
          alignas(Vec::vector_alignment()) std::array<typename Vec::base_type, Vec::vector_element_count()> buffer_a;
          alignas(Vec::vector_alignment()) std::array<typename Vec::base_type, Vec::vector_element_count()> buffer_b;
          _mm512_store_si512(reinterpret_cast<void*>(buffer_a.data()), vec_a);
          _mm512_store_si512(reinterpret_cast<void*>(buffer_b.data()), vec_b);
          for(std::size_t i = 0; i < Vec::vector_element_count(); ++i) {
            buffer_a[i] /= buffer_b[i];
          }
          return _mm512_load_si512(reinterpret_cast<void const *>(buffer_a.data()));
#INTEL - AVX2
  - target_extension: "avx2"
    ctype: ["float", "double"]
    lscpu_flags: ['avx']
    implementation: "return _mm256_div_{{intrin_tp_full[ctype]}}(vec_a, vec_b);"
  - target_extension: "avx2"
    ctype: ["uint8_t", "int8_t", "uint16_t", "int16_t", "uint32_t", "int32_t", "uint64_t", "int64_t"]
    lscpu_flags: ['avx']
    is_native: False
    includes: ["<array>", "<cstddef>"]
    implementation: |
        alignas(Vec::vector_alignment()) std::array<typename Vec::base_type, Vec::vector_element_count()> buffer_a;
        alignas(Vec::vector_alignment()) std::array<typename Vec::base_type, Vec::vector_element_count()> buffer_b;
        _mm256_store_si256(reinterpret_cast<__m256i*>(buffer_a.data()), vec_a);
        _mm256_store_si256(reinterpret_cast<__m256i*>(buffer_b.data()), vec_b);
        for(std::size_t i = 0; i < Vec::vector_element_count(); ++i) {
           buffer_a[i] /= buffer_b[i];
        }
        return _mm256_load_si256(reinterpret_cast<__m256i const *>(buffer_a.data()));
#INTEL - SSE
  - target_extension: "sse"
    ctype: ["float", "double"]
    lscpu_flags: ['sse']
    implementation: "return _mm_div_{{intrin_tp_full[ctype]}}(vec_a, vec_b);"
  - target_extension: "sse"
    ctype: ["uint8_t", "int8_t", "uint16_t", "int16_t", "uint32_t", "int32_t", "uint64_t", "int64_t"]
    lscpu_flags: ['sse2']
    is_native: False
    includes: ["<array>", "<cstddef>"]
    implementation: |
        alignas(Vec::vector_alignment()) std::array<typename Vec::base_type, Vec::vector_element_count()> buffer_a;
        alignas(Vec::vector_alignment()) std::array<typename Vec::base_type, Vec::vector_element_count()> buffer_b;
        _mm_store_si128(reinterpret_cast<__m128i*>(buffer_a.data()), vec_a);
        _mm_store_si128(reinterpret_cast<__m128i*>(buffer_b.data()), vec_b);
        for(std::size_t i = 0; i < Vec::vector_element_count(); ++i) {
          buffer_a[i] /= buffer_b[i];
        }
        return _mm_load_si128(reinterpret_cast<__m128i const *>(buffer_a.data()));
#ARM - NEON - TODO: Cant check if Correct
  - target_extension: "neon"
    ctype: ["float", "double"]
    lscpu_flags: [ 'neon' ]
    implementation: "return vdivq_{{ intrin_tp_full[ctype] }}( vec_a, vec_b );"
  - target_extension: "neon"
    ctype: ["uint64_t"]
    lscpu_flags: ['neon']
    implementation: |
      typename Vec::register_type recipocal_estimate = vshrq_n_u64(vdupq_n_u64(0xFFFFFFFFFFFFFFFF), vclzq_u64(vec_b));
      recipocal_estimate = vmulq_u64(recipocal_estimate, vsubq_u64(vdupq_n_u64(2), vmulq_u64(vec_b, recipocal_estimate)));
      typename Vec::register_type temp = vmulq_u64(vec_a, recipocal_estimate);
      temp = vqrdmulhq_u64(temp, vec_b);
      return temp;
  - target_extension: "neon"
    ctype: ["uint32_t"]
    lscpu_flags: ['neon']
    is_native: False
    implementation: |
      typename Vec::register_type temp = vrecpe_{{intrin_tp_full[ctype]}}(vec_b);
      temp = vmulq_{{intrin_tp_full[ctype]}}(temp, vrecpsq_{{intrin_tp_full[ctype]}}(vec_b, temp));
      return vmulq_{{intrin_tp_full[ctype]}}(vec_a, temp);
  - target_extension: "neon"
    ctype: ["uint16_t"]
    lscpu_flags: ['neon']
    is_native: False
    implementation: return vdivq_u16(vec_a, vdupq_n_u16(vec_b));
  - target_extension: "neon"
    ctype: ["uint8_t"]
    lscpu_flags: ['neon']
    is_native: False
    implementation: |
      typename Vec::register_type temp = vmol_u8(vec_a);
      temp = vmulq_n_u16(temp, 0x100);
      typename Vec::register_type result = vqrdmulhq_u8(temp, vec_b);
      result = vshrn_n_u16(result, 8);
      return result;
#SCALAR
  - target_extension: "scalar"
    ctype: [ "uint8_t", "uint16_t", "uint32_t", "uint64_t", "int8_t", "int16_t", "int32_t", "int64_t", "float", "double" ]
    lscpu_flags: [ ]
    implementation: return vec_a / vec_b;
#INTEL - FPGA
...
---
primitive_name: 'mod'
brief_description: 'Operates the modulo operation on one datavector modulo one input value.'
parameters:
  - ctype: 'const typename Vec::register_type'
    name: 'vec'
    description: 'Input Vector'
  - ctype: 'const typename Vec::base_type'
    name: 'val'
    description: 'Modulo value'
returns:
  ctype: 'typename Vec::register_type'
  description: 'Resulting Vector'
testing:
  - test_name: "with_modulo_one"
    requires: ["storeu", "set1"]
    implementation: |
      using T = typename Vec::base_type;
      testing::test_memory_helper_t<Vec> test_helper{Vec::vector_element_count(), false};
      auto reference_result_ptr = test_helper.result_ref();
      auto test_result_ptr = test_helper.result_target();
      auto vec = set1<Vec>(0);
      storeu<Vec>(reference_result_ptr, vec);
      storeu<Vec>(test_result_ptr, mod<Vec>(vec, 1));
      test_helper.synchronize();
      return test_helper.validate();
  - test_name: "with_check_barrett_reduction_constraint"
    requires: [ "storeu", "set1"]
    implementation: |
      using T = typename Vec::base_type;
      testing::test_memory_helper_t<Vec> test_helper{Vec::vector_element_count(), false};
      auto reference_result_ptr = test_helper.result_ref();
      auto test_result_ptr = test_helper.result_target();
      auto vec = set1<Vec>(73);
      storeu<Vec>(reference_result_ptr, set1<Vec>(1));
      auto result_vec = mod<Vec>(vec, 8);
      storeu<Vec>(test_result_ptr, result_vec);
      test_helper.synchronize();
      return test_helper.validate();
  - test_name: "with_modulo_rand"
    requires: ["loadu", "storeu"]
    implementation: |
      using T = typename Vec::base_type;
      using IntType = std::conditional_t<std::is_same_v<T, float>, int32_t, std::conditional_t<std::is_same_v<T, double>, int64_t, T>>;
      std::size_t element_count = 1024 + (1024 / Vec::vector_element_count());
      testing::test_memory_helper_t<Vec> test_helper{element_count, Vec::vector_element_count(), false, testing::alternate_init_no_zero<T>};
      bool allOk = true;
      auto reference_data_ptr = test_helper.data_ref();
      auto reference_result_ptr = test_helper.result_ref();
      auto test_data_ptr = test_helper.data_target();
      auto test_result_ptr = test_helper.result_target();
      auto offset = 1024;
      for(std::size_t i = 0; i < (element_count - offset); i +=Vec::vector_element_count()){
        std::size_t tester_idx = 0;
        for(std::size_t j = i; j < i + Vec::vector_element_count(); j++){
          reference_result_ptr[tester_idx++] = reference_data_ptr[j] % reference_data_ptr[offset];
        }
        auto vec = loadu<Vec>(&test_data_ptr[i]);
        auto val = test_data_ptr[offset];
        auto vec_result = mod<Vec>(vec, val);
        storeu<Vec>(test_result_ptr, vec_result);
        test_helper.synchronize();
        allOk &= test_helper.validate();
      }
      return allOk;
definitions:
# INTEL - AVX512
  - target_extension: 'avx512'
    ctype: [ 'uint8_t', 'int8_t', 'uint16_t', 'int16_t' ]
    lscpu_flags: [ "avx512f" ]
    includes: [ "<array>", "<cstddef>", "<cmath>" ]
    implementation: |
      alignas(Vec::vector_alignment()) std::array<typename Vec::base_type, Vec::vector_element_count()> buffer;
      _mm512_store_si512(reinterpret_cast<__m512i*>(buffer.data()), vec);
      for (std::size_t i = 0; i < Vec::vector_element_count(); ++i) {
        buffer[i] = buffer[i] % val;
      }
      return _mm512_load_si512(reinterpret_cast<__m512i const*>(buffer.data()));
  - target_extension: 'avx512'
    ctype: [ 'int32_t', "uint32_t" ]
    lscpu_flags: [ "avx512f" ]
    includes: []
    implementation: |
      using T = float;

      __m512 vec_d = tsl::cast<Vec, typename Vec::template transform_extension<T>>(vec);
      __m512 val_d = _mm512_set1_ps(static_cast<T>(val));

      __m512 temp = tsl::div<typename Vec::template transform_extension<T>>(vec_d, val_d);
      temp = _mm512_roundscale_ps(temp, _MM_FROUND_TO_ZERO);
      temp = _mm512_mul_ps(temp, val_d);
      temp = _mm512_sub_ps(vec_d, temp);

      return tsl::cast<typename Vec::template transform_extension<T>,Vec>(temp);
  - target_extension: 'avx512'
    ctype: [ 'int64_t', "uint64_t" ]
    lscpu_flags: [ "avx512f", "avx512dq" ]
    includes: []
    implementation: |
      using T = double;

      __m512d vec_d = tsl::cast<Vec, typename Vec::template transform_extension<T>>(vec);
      __m512d val_d = _mm512_set1_pd(static_cast<T>(val));

      __m512d temp = tsl::div<typename Vec::template transform_extension<T>>(vec_d, val_d);
      temp = _mm512_roundscale_pd(temp, _MM_FROUND_TO_ZERO);
      temp = _mm512_mul_pd(temp, val_d);
      temp = _mm512_sub_pd(vec_d, temp);

      return tsl::cast<typename Vec::template transform_extension<T>,Vec>(temp);
# INTEL - AVX2
  - target_extension: 'avx2'
    ctype: [ 'uint8_t', 'int8_t', "uint16_t", "int16_t" ]
    lscpu_flags: [ "avx2" ]
    includes: ["<array>", "<cstddef>", "<cmath>"]
    is_native: false
    implementation: |
      alignas(Vec::vector_alignment()) std::array<typename Vec::base_type, Vec::vector_element_count()> buffer;
      _mm256_store_si256(reinterpret_cast<__m256i*>(buffer.data()), vec);
      for (std::size_t i = 0; i < Vec::vector_element_count(); ++i) {
        buffer[i] = buffer[i] % val;
      }
      return _mm256_load_si256(reinterpret_cast<__m256i const*>(buffer.data()));
  - target_extension: 'avx2'
    ctype: [ 'uint32_t', 'int32_t' ]
    lscpu_flags: [ "avx2" ]
    includes: [ ]
    implementation: |
      using T = float;

      __m256 vec_d = tsl::cast<Vec, typename Vec::template transform_extension<T>>(vec);
      __m256 val_d = _mm256_set1_ps(static_cast<T>(val));

      __m256 temp = tsl::div<typename Vec::template transform_extension<T>>(vec_d, val_d);
      temp = _mm256_round_ps(temp, _MM_FROUND_TO_ZERO);
      temp = _mm256_mul_ps(temp, val_d);
      temp = _mm256_sub_ps(vec_d, temp);

      return tsl::cast<typename Vec::template transform_extension<T>,Vec>(temp);
  - target_extension: 'avx2'
    ctype: [ 'uint64_t', 'int64_t' ]
    lscpu_flags: [ "avx2" ]
    includes: [ ]
    implementation: |
      using T = double;

      __m256d vec_d = tsl::cast<Vec, typename Vec::template transform_extension<T>>(vec);
      __m256d val_d = _mm256_set1_pd(static_cast<T>(val));

      __m256d temp = tsl::div<typename Vec::template transform_extension<T>>(vec_d, val_d);
      temp = _mm256_round_pd(temp, _MM_FROUND_TO_ZERO);
      temp = _mm256_mul_pd(temp, val_d);
      temp = _mm256_sub_pd(vec_d, temp);

      return tsl::cast<typename Vec::template transform_extension<T>,Vec>(temp);
# INTEL - SSE
  - target_extension: 'sse'
    ctype: [ 'uint8_t', 'int8_t', 'uint16_t', 'int16_t']
    lscpu_flags: [ "sse2" ]
    includes: ["<array>", "<cstddef>", "<cmath>"]
    is_native: false
    implementation: |
      alignas(Vec::vector_alignment()) std::array<typename Vec::base_type, Vec::vector_element_count()> buffer;
      _mm_store_si128(reinterpret_cast<__m128i*>(buffer.data()), vec);
      for (std::size_t i = 0; i < Vec::vector_element_count(); ++i) {
        buffer[i] = buffer[i] % val;
      }
      return _mm_load_si128(reinterpret_cast<__m128i const*>(buffer.data()));
  - target_extension: 'sse'
    ctype: [ 'int32_t', "uint32_t" ]
    lscpu_flags: [ "sse2", "sse4_1" ]
    includes: []
    implementation: |
      using T = float;
      __m128 vec_d = tsl::cast<Vec, typename Vec::template transform_extension<T>>(vec);
      __m128 val_d = _mm_set1_ps(static_cast<T>(val));

      __m128 temp = tsl::div<typename Vec::template transform_extension<T>>(vec_d, val_d);
      temp = _mm_round_ps(temp, _MM_FROUND_TO_ZERO);
      temp = _mm_mul_ps(temp, val_d);
      temp = _mm_sub_ps(vec_d, temp);

      return tsl::cast<typename Vec::template transform_extension<T>,Vec>(temp);
  - target_extension: 'sse'
    ctype: [ 'int64_t', "uint64_t" ]
    lscpu_flags: [ "sse2", "sse4_1" ]
    includes: []
    implementation: |
      using T = double;

      __m128d vec_d = tsl::cast<Vec, typename Vec::template transform_extension<T>>(vec);
      __m128d val_d = _mm_set1_pd(static_cast<T>(val));

      __m128d temp = tsl::div<typename Vec::template transform_extension<T>>(vec_d, val_d);
      temp = _mm_round_pd(temp, _MM_FROUND_TO_ZERO);
      temp = _mm_mul_pd(temp, val_d);
      temp = _mm_sub_pd(vec_d, temp);

      return tsl::cast<typename Vec::template transform_extension<T>,Vec>(temp);
# SCALAR
  - target_extension: 'scalar'
    ctype: [ 'uint8_t', 'uint16_t', 'uint32_t', 'uint64_t', 'int8_t', 'int16_t', 'int32_t', 'int64_t' ]
    lscpu_flags: [ ]
    implementation: return vec % val;
...
---
primitive_name: 'mod_barrett'
brief_description: |
  If the barrett condition holds (with a mod n => a < n) 
  this Primitive correctly operates the modulo operation on one datavector modulo one input value.
  Note: Barrett Reduction is definied for unsigned values.
parameters:
  - ctype: 'const typename Vec::register_type'
    name: 'vec'
    description: 'Input Vector'
  - ctype: 'const typename Vec::base_type'
    name: 'val'
    description: 'Modulo value'
returns:
  ctype: 'typename Vec::register_type'
  description: 'Resulting Vector'
testing:
  - test_name: "with_modulo_one"
    requires: ["storeu", "set1"]
    implementation: |
      using T = typename Vec::base_type;
      testing::test_memory_helper_t<Vec> test_helper{Vec::vector_element_count(), false};
      auto reference_result_ptr = test_helper.result_ref();
      auto test_result_ptr = test_helper.result_target();
      auto vec = set1<Vec>(0);
      storeu<Vec>(reference_result_ptr, vec);
      storeu<Vec>(test_result_ptr, mod_barrett<Vec>(vec, 1));
      test_helper.synchronize();
      return test_helper.validate();
  - test_name: "with_modulo_rand"
    requires: ["loadu", "storeu"]
    implementation: |
      using T = typename Vec::base_type;
      std::size_t offset = 1024;
      std::size_t element_count = offset + (offset / Vec::vector_element_count());
      testing::test_memory_helper_t<Vec> test_helper{element_count, Vec::vector_element_count(), false, testing::alternate_init_no_zero<T>};
      bool allOk = true;
      auto reference_data_ptr = test_helper.data_ref();
      auto reference_result_ptr = test_helper.result_ref();
      auto test_data_ptr = test_helper.data_target();
      auto test_result_ptr = test_helper.result_target();
      int test = 0;
      for(std::size_t i = 0; i < (element_count - offset); i += Vec::vector_element_count()){
        std::size_t tester_idx = 0;
        for(std::size_t j = i; j < i + Vec::vector_element_count(); j++){
          reference_result_ptr[tester_idx++] = reference_data_ptr[j] % reference_data_ptr[offset];
        }
        auto vec = loadu<Vec>(&test_data_ptr[i]);
        auto val = test_data_ptr[offset];
        auto vec_result = mod_barrett<Vec>(vec, val);
        storeu<Vec>(test_result_ptr, vec_result);
        test_helper.synchronize();
        bool a = test_helper.validate();
        allOk &= a;

        // Check if the barrett condition did not hold.
        if(!a){
          for(std::size_t j = i; j < i + Vec::vector_element_count(); j++){
            if(!(reference_data_ptr[j] < (reference_data_ptr[offset] * reference_data_ptr[offset]))){
              allOk = true;
            }
          }
        }
        offset++;
      }
      return allOk;
definitions:
# INTEL - AVX512
  - target_extension: ["avx512"]
    ctype: ["uint8_t"]
    lscpu_flags: ["avx512f", "avx512vl"]
    implementation: |
      using T = uint16_t;
      __m512i mu_vec, q, val_vec, r, cmp, vec_lo, vec_hi, temp_lo, temp_hi, mask;

      // Compute Barrett constants
      T mu = (1 << 8) / val;
      mu_vec = _mm512_set1_epi16(mu);
      val_vec = _mm512_set1_epi16(val);

      // Zero extend and split vector
      __mmask32 mask_split = 0xFFFFFFFF;
      vec_lo = _mm512_maskz_cvtepu8_epi16(mask_split, _mm512_extracti64x4_epi64(vec, 0));
      vec_hi = _mm512_maskz_cvtepu8_epi16(mask_split, _mm512_extracti64x4_epi64(vec, 1)); 
      
      // Barrett reduction
      temp_lo = tsl::mul<typename Vec::template transform_extension<T>>(vec_lo, mu_vec);
      temp_hi = tsl::mul<typename Vec::template transform_extension<T>>(vec_hi, mu_vec);

      temp_lo = _mm512_srli_epi16(temp_lo, 8);
      temp_hi = _mm512_srli_epi16(temp_hi, 8);

      temp_lo = tsl::mul<typename Vec::template transform_extension<T>>(temp_lo, val_vec);
      temp_hi = tsl::mul<typename Vec::template transform_extension<T>>(temp_hi, val_vec);

      // Calculate the remainder
      vec_lo = _mm512_sub_epi16(vec_lo, temp_lo);
      vec_hi = _mm512_sub_epi16(vec_hi, temp_hi);

      // Handle remainders greater than the input value
      __mmask32 cmp_lo = _mm512_cmpge_epi16_mask(vec_lo, val_vec);
      __mmask32 cmp_hi = _mm512_cmpge_epi16_mask(vec_hi, val_vec);
  
      vec_lo = _mm512_sub_epi16(vec_lo, _mm512_maskz_mov_epi16(cmp_lo, val_vec));
      vec_hi = _mm512_sub_epi16(vec_hi, _mm512_maskz_mov_epi16(cmp_hi, val_vec));

      // Convert back to 8-Bit values and put together
      return _mm512_inserti64x4(_mm512_castsi256_si512(_mm512_cvtepi16_epi8(vec_lo)), _mm512_cvtepi16_epi8(vec_hi),1);
  - target_extension: ["avx512"]
    ctype: ["uint16_t"]
    lscpu_flags: ["avx512f", "avx512vl"]
    implementation: |
      using T = uint32_t;
      __m512i mu_vec, q, val_vec, r, cmp, vec_lo, vec_hi, temp_lo, temp_hi, mask;

      // Compute Barrett constants
      T mu = (1 << 16) / val;
      mu_vec = _mm512_set1_epi32(mu);
      val_vec = _mm512_set1_epi32(val);

      // Zero extend and split vector
      __mmask16 mask_split = 0xFFFF;
      vec_lo = _mm512_maskz_cvtepu16_epi32(mask_split, _mm512_extracti64x4_epi64(vec, 0));
      vec_hi = _mm512_maskz_cvtepu16_epi32(mask_split, _mm512_extracti64x4_epi64(vec, 1)); 

      // Barrett reduction
      temp_lo = tsl::mul<typename Vec::template transform_extension<T>>(vec_lo, mu_vec);
      temp_hi = tsl::mul<typename Vec::template transform_extension<T>>(vec_hi, mu_vec);

      temp_lo = _mm512_srli_epi32(temp_lo, 16);
      temp_hi = _mm512_srli_epi32(temp_hi, 16);

      temp_lo = tsl::mul<typename Vec::template transform_extension<T>>(temp_lo, val_vec);
      temp_hi = tsl::mul<typename Vec::template transform_extension<T>>(temp_hi, val_vec);

      // Calculate the remainder
      vec_lo = _mm512_sub_epi32(vec_lo, temp_lo);
      vec_hi = _mm512_sub_epi32(vec_hi, temp_hi);

      // Handle remainders greater than the input value
      __mmask16 cmp_lo = _mm512_cmpge_epi32_mask(vec_lo, val_vec);
      __mmask16 cmp_hi = _mm512_cmpge_epi32_mask(vec_hi, val_vec);
  
      vec_lo = _mm512_sub_epi32(vec_lo, _mm512_maskz_mov_epi32(cmp_lo, val_vec));
      vec_hi = _mm512_sub_epi32(vec_hi, _mm512_maskz_mov_epi32(cmp_hi, val_vec));

      // Convert back to 8-Bit values and put together
      return _mm512_inserti64x4(_mm512_castsi256_si512(_mm512_cvtepi32_epi16(vec_lo)), _mm512_cvtepi32_epi16(vec_hi),1);
  - target_extension: ["avx512"]
    ctype: ["uint32_t"]
    lscpu_flags: ["avx512f", "avx512vl"]
    implementation: |
      using T = uint64_t;
      __m512i mu_vec, q, val_vec, r, cmp, vec_lo, vec_hi, temp_lo, temp_hi, mask;

      // Compute Barrett constants
      T mu = (1 << 32) / val;
      mu_vec = _mm512_set1_epi64(mu);
      val_vec = _mm512_set1_epi64(val);

      // Zero extend and split vector
      __mmask8 mask_split = 0xFF;
      vec_lo = _mm512_maskz_cvtepu32_epi64(mask_split, _mm512_extracti64x4_epi64(vec, 0));
      vec_hi = _mm512_maskz_cvtepu32_epi64(mask_split, _mm512_extracti64x4_epi64(vec, 1)); 

      // Barrett reduction
      temp_lo = tsl::mul<typename Vec::template transform_extension<T>>(vec_lo, mu_vec);
      temp_hi = tsl::mul<typename Vec::template transform_extension<T>>(vec_hi, mu_vec);

      temp_lo = _mm512_srli_epi32(temp_lo, 32);
      temp_hi = _mm512_srli_epi32(temp_hi, 32);

      temp_lo = tsl::mul<typename Vec::template transform_extension<T>>(temp_lo, val_vec);
      temp_hi = tsl::mul<typename Vec::template transform_extension<T>>(temp_hi, val_vec);

      // Calculate the remainder
      vec_lo = _mm512_sub_epi64(vec_lo, temp_lo);
      vec_hi = _mm512_sub_epi64(vec_hi, temp_hi);

      // Handle remainders greater than the input value
      __mmask8 cmp_lo = _mm512_cmpge_epi64_mask(vec_lo, val_vec);
      __mmask8 cmp_hi = _mm512_cmpge_epi64_mask(vec_hi, val_vec);
  
      vec_lo = _mm512_sub_epi64(vec_lo, _mm512_maskz_mov_epi64(cmp_lo, val_vec));
      vec_hi = _mm512_sub_epi64(vec_hi, _mm512_maskz_mov_epi64(cmp_hi, val_vec));

      // Convert back to 8-Bit values and put together
      return _mm512_inserti64x4(_mm512_castsi256_si512(_mm512_cvtepi64_epi32(vec_lo)), _mm512_cvtepi64_epi32(vec_hi),1);
  - target_extension: ["avx512"]
    ctype: ["uint64_t"]
    lscpu_flags: ["avx512f", "avx512vl"]
    implementation: |
      __m512i mu_vec, val_vec, q1, q2, q3, r1, r2, r3, r;

      val_vec = _mm512_set1_epi64(val);

      // Compute Barrett Constant
      uint64_t mu = ((1ULL << 64) + val - 1) / val;
      mu_vec = _mm512_set1_epi64(mu);

      q1 = _mm512_srli_epi64(vec, 32);
      q1 = tsl::mul<Vec>(q1, mu_vec);
      q1 = _mm512_srli_epi64(q1, 32);
      q2 = tsl::mul<Vec>(q1, val_vec);

      r1 = _mm512_and_epi64(vec, _mm512_set1_epi64(0xFFFFFFFF));
      r2 = tsl::mul<Vec>(mu_vec, r1);
      q3 = _mm512_srli_epi64(r2, 32);

      r3 = tsl::mul<Vec>(q3, val_vec);
      r3 = _mm512_sub_epi64(r1, r3);

      r = _mm512_srli_epi64(r3, 63);
      r = _mm512_and_epi64(r, val_vec);
      r = _mm512_add_epi64(r, r3);

      // Handle remainders greater than the input value
      __mmask8 cmp = _mm512_cmpge_epi64_mask(r, val_vec);
      r = _mm512_sub_epi64(r, _mm512_maskz_mov_epi64(cmp, val_vec));
      return r;
# INTEL - AVX2
  - target_extension: ["avx2"]
    ctype: ["uint8_t"]
    lscpu_flags: ["avx", "avx2"]
    implementation: |
      using T = uint16_t;
      __m256i mu_vec, val_vec, vec_lo, vec_hi, temp_lo, temp_hi, mask;

      // Compute Barrett constants
      T mu = (1 << 8) / val;
      mu_vec = _mm256_set1_epi16(mu);
      val_vec = _mm256_set1_epi16(val);

      __m128i lower = _mm256_extracti128_si256(vec, 0);
      __m128i upper = _mm256_extracti128_si256(vec, 1);

      vec_lo = _mm256_cvtepu8_epi16(lower);
      vec_hi = _mm256_cvtepu8_epi16(upper);

      // Barrett reduction
      temp_lo = tsl::mul<typename Vec::template transform_extension<T>>(vec_lo, mu_vec);
      temp_hi = tsl::mul<typename Vec::template transform_extension<T>>(vec_hi, mu_vec);

      temp_lo = _mm256_srli_epi16(temp_lo, 8);
      temp_hi = _mm256_srli_epi16(temp_hi, 8);

      temp_lo = tsl::mul<typename Vec::template transform_extension<T>>(temp_lo, val_vec);
      temp_hi = tsl::mul<typename Vec::template transform_extension<T>>(temp_hi, val_vec);

      // Calculate the remainder
      vec_lo = _mm256_sub_epi16(vec_lo, temp_lo);
      vec_hi = _mm256_sub_epi16(vec_hi, temp_hi);

      // Handle remainders greater than the input value
      temp_lo = _mm256_or_si256(_mm256_cmpgt_epi16(vec_lo, val_vec), _mm256_cmpeq_epi16(vec_lo, val_vec));
      temp_hi = _mm256_or_si256(_mm256_cmpgt_epi16(vec_hi, val_vec), _mm256_cmpeq_epi16(vec_hi, val_vec));

      vec_lo = _mm256_sub_epi16(vec_lo, _mm256_and_si256(temp_lo, val_vec));
      vec_hi = _mm256_sub_epi16(vec_hi, _mm256_and_si256(temp_hi, val_vec));

      // Put back into 8-Bit values
      vec_lo = _mm256_packus_epi16(vec_lo, _mm256_setzero_si256());
      vec_hi = _mm256_packus_epi16(vec_hi, _mm256_setzero_si256());
        
      // Shuffle values to the right place
      vec_lo = _mm256_permute4x64_epi64(vec_lo, _MM_SHUFFLE(3,1,2,0));
      vec_hi = _mm256_permute4x64_epi64(vec_hi, _MM_SHUFFLE(2,0,3,1));
    
      return _mm256_or_si256(vec_lo, vec_hi);
  - target_extension: ["avx2"]
    ctype: ["uint16_t"]
    lscpu_flags: ["avx", "avx2"]
    implementation: |
      using T = uint32_t;
      __m256i mu_vec, val_vec, vec_lo, vec_hi, temp_lo, temp_hi, mask;

      // Compute Barrett constants
      T mu = (1 << 16) / val;
      mu_vec = _mm256_set1_epi32(mu);
      val_vec = _mm256_set1_epi32(val);

      vec_lo = _mm256_cvtepu16_epi32(_mm256_extractf128_si256(vec, 0));
      vec_hi = _mm256_cvtepu16_epi32(_mm256_extractf128_si256(vec, 1));

      // Barrett reduction
      temp_lo = tsl::mul<typename Vec::template transform_extension<T>>(vec_lo, mu_vec);
      temp_hi = tsl::mul<typename Vec::template transform_extension<T>>(vec_hi, mu_vec);

      temp_lo = _mm256_srli_epi32(temp_lo, 16);
      temp_hi = _mm256_srli_epi32(temp_hi, 16);

      temp_lo = tsl::mul<typename Vec::template transform_extension<T>>(temp_lo, val_vec);
      temp_hi = tsl::mul<typename Vec::template transform_extension<T>>(temp_hi, val_vec);

      // Calculate the remainder
      vec_lo = _mm256_sub_epi32(vec_lo, temp_lo);
      vec_hi = _mm256_sub_epi32(vec_hi, temp_hi);

      // Handle remainders greater than the input value
      temp_lo = _mm256_or_si256(_mm256_cmpgt_epi32(vec_lo, val_vec), _mm256_cmpeq_epi32(vec_lo, val_vec));
      temp_hi = _mm256_or_si256(_mm256_cmpgt_epi32(vec_hi, val_vec), _mm256_cmpeq_epi32(vec_hi, val_vec));

      vec_lo = _mm256_sub_epi32(vec_lo, _mm256_and_si256(temp_lo, val_vec));
      vec_hi = _mm256_sub_epi32(vec_hi, _mm256_and_si256(temp_hi, val_vec));

      // Put back into 16-Bit values
      vec_lo = _mm256_packus_epi32(vec_lo, _mm256_setzero_si256());
      vec_hi = _mm256_packus_epi32(vec_hi, _mm256_setzero_si256());
        
      // Shuffle values to the right place
      vec_lo = _mm256_permute4x64_epi64(vec_lo, _MM_SHUFFLE(3,1,2,0));
      vec_hi = _mm256_permute4x64_epi64(vec_hi, _MM_SHUFFLE(2,0,3,1));
    
      return _mm256_or_si256(vec_lo, vec_hi);
  - target_extension: ["avx2"]
    ctype: ["uint32_t"]
    lscpu_flags: ["avx", "avx2"]
    implementation: |
      using T = uint64_t;
      __m256i mu_vec, val_vec, vec_lo, vec_hi, temp_lo, temp_hi, mask;

      // Compute Barrett constants
      T mu = (1 << 32) / val;
      mu_vec = _mm256_set1_epi64x(mu);
      val_vec = _mm256_set1_epi64x(val);

      vec_lo = _mm256_cvtepu32_epi64(_mm256_extractf128_si256(vec, 0));
      vec_hi = _mm256_cvtepu32_epi64(_mm256_extractf128_si256(vec, 1));

      // Barrett reduction
      temp_lo = tsl::mul<typename Vec::template transform_extension<T>>(vec_lo, mu_vec);
      temp_hi = tsl::mul<typename Vec::template transform_extension<T>>(vec_hi, mu_vec);

      temp_lo = _mm256_srli_epi64(temp_lo, 32);
      temp_hi = _mm256_srli_epi64(temp_hi, 32);

      temp_lo = tsl::mul<typename Vec::template transform_extension<T>>(temp_lo, val_vec);
      temp_hi = tsl::mul<typename Vec::template transform_extension<T>>(temp_hi, val_vec);

      // Calculate the remainder
      vec_lo = _mm256_sub_epi64(vec_lo, temp_lo);
      vec_hi = _mm256_sub_epi64(vec_hi, temp_hi);

      // Handle remainders greater than the input value
      temp_lo = _mm256_or_si256(_mm256_cmpgt_epi64(vec_lo, val_vec), _mm256_cmpeq_epi64(vec_lo, val_vec));
      temp_hi = _mm256_or_si256(_mm256_cmpgt_epi64(vec_hi, val_vec), _mm256_cmpeq_epi64(vec_hi, val_vec));

      vec_lo = _mm256_sub_epi64(vec_lo, _mm256_and_si256(temp_lo, val_vec));
      vec_hi = _mm256_sub_epi64(vec_hi, _mm256_and_si256(temp_hi, val_vec));

      // Shuffle values to the right place
      vec_lo = _mm256_permutevar8x32_epi32(vec_lo, _mm256_set_epi32(7, 5, 3, 1, 6, 4, 2, 0));
      vec_hi = _mm256_permutevar8x32_epi32(vec_hi, _mm256_set_epi32(6, 4, 2, 0, 7, 5, 3, 1));
    
      return _mm256_or_si256(vec_lo, vec_hi);
  - target_extension: ["avx2"]
    ctype: ["uint64_t"]
    lscpu_flags: ["avx2", "avx"]
    implementation: |
      __m256i mu_vec, val_vec, q1, q2, q3, r1, r2, r3, r, cmp;

      val_vec = _mm256_set1_epi64x(val);

      // Compute Barrett Constant
      uint64_t mu = ((1ULL << 64) + val - 1) / val;
      mu_vec = _mm256_set1_epi64x(mu);

      q1 = _mm256_srli_si256(vec, 32);
      q1 = tsl::mul<Vec>(q1, mu_vec);
      q1 = _mm256_srli_si256(q1, 32);
      q2 = tsl::mul<Vec>(q1, val_vec);

      r1 = _mm256_and_si256(vec, _mm256_set1_epi64x(0xFFFFFFFF));
      r2 = tsl::mul<Vec>(mu_vec, r1);
      q3 = _mm256_srli_si256(r2, 32);


      r3 = tsl::mul<Vec>(q3, val_vec);
      r3 = _mm256_sub_epi64(r1, r3);

      r = _mm256_srli_si256(r3, 63);
      r = _mm256_and_si256(r, val_vec);
      r = _mm256_add_epi64(r, r3);

      // Handle remainders greater than the input value
      cmp = _mm256_or_si256(_mm256_cmpgt_epi64(r, val_vec), _mm256_cmpeq_epi64(r, val_vec));
      r = _mm256_sub_epi64(r, _mm256_and_si256(cmp, val_vec));

      return r;
# INTEL - SSE
  - target_extension: ["sse"]
    ctype: ["uint8_t"]
    lscpu_flags: ["sse2"]
    implementation: |
      using T = uint16_t;
      __m128i mu_vec, q, val_vec, r, cmp, vec_lo, vec_hi, temp_lo, temp_hi, mask;
     
      // Compute Barrett constants
      T mu = (1 << 8) / val;
      mu_vec = _mm_set1_epi16(mu);
      val_vec = _mm_set1_epi16(val);
      
      vec_lo = _mm_cvtepu8_epi16(vec);
      vec_hi = _mm_cvtepu8_epi16(_mm_srli_si128(vec, 8));

      // Barrett reduction
      temp_lo = tsl::mul<typename Vec::template transform_extension<T>>(vec_lo, mu_vec);
      temp_hi = tsl::mul<typename Vec::template transform_extension<T>>(vec_hi, mu_vec);

      temp_lo = _mm_srli_epi16(temp_lo, 8);
      temp_hi = _mm_srli_epi16(temp_hi, 8);

      temp_lo = tsl::mul<typename Vec::template transform_extension<T>>(temp_lo, val_vec);
      temp_hi = tsl::mul<typename Vec::template transform_extension<T>>(temp_hi, val_vec);
      
      // Calculate the remainder
      vec_lo = _mm_sub_epi16(vec_lo, temp_lo);
      vec_hi = _mm_sub_epi16(vec_hi, temp_hi);
      
      // Handle remainders greater than the input value
      temp_lo = _mm_or_si128(_mm_cmpgt_epi16(vec_lo, val_vec), _mm_cmpeq_epi16(vec_lo, val_vec));
      temp_hi = _mm_or_si128(_mm_cmpgt_epi16(vec_hi, val_vec), _mm_cmpeq_epi16(vec_hi, val_vec));

      vec_lo = _mm_sub_epi16(vec_lo, _mm_and_si128(temp_lo, val_vec));
      vec_hi = _mm_sub_epi16(vec_hi, _mm_and_si128(temp_hi, val_vec));

      // Shuffle 8 bits to the right place
      vec_lo = _mm_shuffle_epi8(vec_lo, _mm_setr_epi8(0, 2, 4, 6, 8, 10, 12, 14, -1, -1, -1, -1, -1, -1, -1, -1));
      vec_hi = _mm_shuffle_epi8(vec_hi, _mm_setr_epi8(-1, -1, -1, -1, -1, -1, -1, -1, 0, 2, 4, 6, 8, 10, 12, 14));

      return _mm_or_si128(vec_lo, vec_hi);
  - target_extension: ["sse"]
    ctype: [ "uint16_t" ]
    lscpu_flags: ["sse2"]
    implementation: |
      using T = uint32_t;
      __m128i mu_vec, q, val_vec, r, cmp, vec_lo, vec_hi, temp_lo, temp_hi, mask;
     
      // Compute Barrett constants
      T mu = (1 << 16) / val;
      mu_vec = _mm_set1_epi32(mu);
      val_vec = _mm_set1_epi32(val);
      
      vec_lo = _mm_cvtepu16_epi32(vec);
      vec_hi = _mm_cvtepu16_epi32(_mm_srli_si128(vec, 8));

      // Barrett reduction
      temp_lo = tsl::mul<typename Vec::template transform_extension<T>>(vec_lo, mu_vec);
      temp_hi = tsl::mul<typename Vec::template transform_extension<T>>(vec_hi, mu_vec);

      temp_lo = _mm_srli_epi32(temp_lo, 16);
      temp_hi = _mm_srli_epi32(temp_hi, 16);

      temp_lo = tsl::mul<typename Vec::template transform_extension<T>>(temp_lo, val_vec);
      temp_hi = tsl::mul<typename Vec::template transform_extension<T>>(temp_hi, val_vec);
      
      // Calculate the remainder
      vec_lo = _mm_sub_epi32(vec_lo, temp_lo);
      vec_hi = _mm_sub_epi32(vec_hi, temp_hi);
      
      // Handle remainders greater than the input value
      temp_lo = _mm_or_si128(_mm_cmpgt_epi32(vec_lo, val_vec), _mm_cmpeq_epi32(vec_lo, val_vec));
      temp_hi = _mm_or_si128(_mm_cmpgt_epi32(vec_hi, val_vec), _mm_cmpeq_epi32(vec_hi, val_vec));

      vec_lo = _mm_sub_epi32(vec_lo, _mm_and_si128(temp_lo, val_vec));
      vec_hi = _mm_sub_epi32(vec_hi, _mm_and_si128(temp_hi, val_vec));

      // Shuffle values to the right place
      vec_lo = _mm_shuffle_epi8(vec_lo, _mm_setr_epi8(0, 1, 4, 5, 8, 9, 12, 13, -1, -1, -1, -1, -1, -1, -1, -1));
      vec_hi = _mm_shuffle_epi8(vec_hi, _mm_setr_epi8(-1, -1, -1, -1, -1, -1, -1, -1, 0, 1, 4, 5, 8, 9, 12, 13));
      return _mm_or_si128(vec_lo, vec_hi);
  - target_extension: 'sse'
    ctype: [ 'uint32_t' ]
    lscpu_flags: [ 'sse2' ]
    implementation: |
      using T = uint64_t;
      __m128i mu_vec, q, val_vec, r, cmp, vec_lo, vec_hi, temp_lo, temp_hi, mask;
     
      // Compute Barrett constants
      T mu = (1 << 32) / val;
      mu_vec = _mm_set1_epi64x(mu);
      val_vec = _mm_set1_epi64x(val);
      
      vec_lo = _mm_cvtepu32_epi64(vec);
      vec_hi = _mm_cvtepu32_epi64(_mm_srli_si128(vec, 8));

      // Barrett reduction
      temp_lo = tsl::mul<typename Vec::template transform_extension<T>>(vec_lo, mu_vec);
      temp_hi = tsl::mul<typename Vec::template transform_extension<T>>(vec_hi, mu_vec);

      temp_lo = _mm_srli_epi64(temp_lo, 32);
      temp_hi = _mm_srli_epi64(temp_hi, 32);

      temp_lo = tsl::mul<typename Vec::template transform_extension<T>>(temp_lo, val_vec);
      temp_hi = tsl::mul<typename Vec::template transform_extension<T>>(temp_hi, val_vec);
      
      // Calculate the remainder
      vec_lo = _mm_sub_epi64(vec_lo, temp_lo);
      vec_hi = _mm_sub_epi64(vec_hi, temp_hi);
      
      // Handle remainders greater than the input value
      temp_lo = _mm_or_si128(_mm_cmpgt_epi64(vec_lo, val_vec), _mm_cmpeq_epi64(vec_lo, val_vec));
      temp_hi = _mm_or_si128(_mm_cmpgt_epi64(vec_hi, val_vec), _mm_cmpeq_epi64(vec_hi, val_vec));

      vec_lo = _mm_sub_epi64(vec_lo, _mm_and_si128(temp_lo, val_vec));
      vec_hi = _mm_sub_epi64(vec_hi, _mm_and_si128(temp_hi, val_vec));

      // Pack the lower 32-bits of each 64-bit integer back into the lower half of the vectors
      vec_lo = _mm_shuffle_epi32(vec_lo, _MM_SHUFFLE(2, 0, 2, 0));
      vec_hi = _mm_shuffle_epi32(vec_hi, _MM_SHUFFLE(2, 0, 2, 0));

      // Blend the vectors back together
      return _mm_blend_epi16(vec_lo, vec_hi, 0b11110000);
  - target_extension: 'sse'
    ctype: [ 'uint64_t' ]
    lscpu_flags: [ 'sse2', 'sse4_1' ]
    implementation: |
      __m128i mu_vec, val_vec, q1, q2, q3, r1, r2, r3, r, cmp;
      
      val_vec = _mm_set1_epi64x(val);

      // Compute Barrett Constant
      uint64_t mu = ((1ULL << 64) + val - 1) / val;
      mu_vec = _mm_set1_epi64x(mu);

      q1 = _mm_srli_si128(vec, 32);
      q1 = tsl::mul<Vec>(q1, mu_vec);
      q1 = _mm_srli_si128(q1, 32);
      q2 = tsl::mul<Vec>(q1, val_vec);

      r1 = _mm_and_si128(vec, _mm_set1_epi64x(0xFFFFFFFF));
      r2 = tsl::mul<Vec>(mu_vec,r1);
      q3 = _mm_srli_si128(r2, 32);
      
      r3 = tsl::mul<Vec>(q3, val_vec);
      r3 = _mm_sub_epi64(r1, r3);

      r = _mm_srli_si128(r3, 63);
      r = _mm_and_si128(r, val_vec);
      r = _mm_add_epi64(r, r3);

      // Handle remainders greater than the input value
      cmp = _mm_or_si128(_mm_cmpgt_epi64(r, val_vec), _mm_cmpeq_epi64(r, val_vec));
      r = _mm_sub_epi64(r, _mm_and_si128(cmp, val_vec));

      return r;
# SCALAR
  - target_extension: 'scalar'
    ctype: [ 'uint8_t', 'uint16_t', 'uint32_t', 'uint64_t']
    lscpu_flags: [ ]
    implementation: |
      Vec::base_type q, a, mu;
      mu = (1 << {{intrin_tp[ctype][1]}}) / val;
      a = vec;

      q = (a * mu) >> {{intrin_tp[ctype][1]}};
      a -= q * val;
      if (val <= a){
        a -= val;
      }
      return a;
...